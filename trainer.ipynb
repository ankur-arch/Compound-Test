{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required python standard libraries\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All torch related imports \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torch import nn, optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cv2 to read an image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sci-kit related imports \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIMS = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_finder(predictions , labels):\n",
    "    values, max_indices = torch.max(predictions, dim=1)\n",
    "    accuracy = ( max_indices == labels ).sum()\n",
    "    return accuracy/max_indices.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_preprocessor(base_dir:str, directory:str):\n",
    "    return os.path.join(base_dir,directory).replace(\"\\\\\",\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_image_list_from_processed_csv(csv_file):\n",
    "    ### This returns the entire list full of images to be loaded into cpu\n",
    "    ###\n",
    "    ###\n",
    "    ALL_IMAGES = []\n",
    "    start = time.time()\n",
    "    for i, items in tq(enumerate(csv_file)):\n",
    "        image = cv2.imread(items, cv2.COLOR_BGR2RGB)\n",
    "        resized = cv2.resize(image,(IMAGE_DIMS,IMAGE_DIMS))\n",
    "        ALL_IMAGES.append(resized)\n",
    "    \n",
    "    print(\"Tt took us approximately {} seconds\".format(time.time()-start))  \n",
    "    return ALL_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoded_labels(input_data_frame):\n",
    "    input_data_frame.labels = input_data_frame.labels.map(lambda x: x-1)\n",
    "    # return pd.get_dummies(input_data_frame.labels, prefix='labels').to_numpy()\n",
    "    return input_data_frame.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch device configurations \n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = pd.read_pickle('./sync_pickles/training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_CSV, X_VAL_CSV,Y_TRAIN, Y_VAL = train_test_split(DATASET.images, DATASET.labels, test_size=0.20, stratify=DATASET.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27551 6888 27551 6888\n"
     ]
    }
   ],
   "source": [
    "print(len(X_TRAIN_CSV),len(X_VAL_CSV),len(Y_TRAIN), len(Y_VAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NORMALIZER = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_DIMS,IMAGE_DIMS)),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeanIsolatedCharacterDataset(Dataset):\n",
    "    def __init__(self, images, labels,  transforms=None):\n",
    "          \n",
    "        ### labels\n",
    "        self.labels = labels.to_numpy()\n",
    "        \n",
    "        self.ALL_IMAGES = images.to_numpy()\n",
    "        \n",
    "        ### transformations to apply on images\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # convert labels to tensor \n",
    "        label = torch.tensor(self.labels[index])\n",
    "        # load single image from list of all preloaded images\n",
    "        image = self.ALL_IMAGES[index]\n",
    "        if self.transforms:\n",
    "            ## apply transforms \n",
    "            image = self.transforms(image)    \n",
    "            image = image.float()\n",
    "        label = label.long()\n",
    "        return image, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ALL_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = LeanIsolatedCharacterDataset(images=X_TRAIN_CSV, labels=Y_TRAIN, transforms=DATA_NORMALIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27551"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAINING_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATASET = LeanIsolatedCharacterDataset(images=X_VAL_CSV, labels=Y_VAL, transforms=DATA_NORMALIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VALIDATION_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LOADER = DataLoader(dataset=TRAINING_DATASET,batch_size=BATCH_SIZE,shuffle=True)\n",
    "VALIDATION_LOADER = DataLoader(dataset=VALIDATION_DATASET,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1,)\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=1, stride=1, padding=0,)\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    #Identity block\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        #x = self.relu(x) #custom \n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels change\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, intermediate_channels * 4, kernel_size=1, stride=stride,),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channel=3, num_classes=171):\n",
    "    return ResNet(block, [2, 3, 5, 2], img_channel, num_classes)\n",
    "net = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  optim.Adam(net.parameters(), lr=0.001) # learning rate \n",
    "# defining the loss function\n",
    "criterion =  nn.CrossEntropyLoss() # reduction='none'\n",
    "net = net.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(ValidationLoader):\n",
    "    total_val_accuracy = 0\n",
    "    total_val_loss = 0 \n",
    "    \n",
    "    for i, data in enumerate(ValidationLoader, 0):\n",
    "       # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_val_loss += loss\n",
    "        val_accuracy = accuracy_finder(predictions=outputs, labels=labels)\n",
    "        total_val_accuracy = total_val_accuracy + val_accuracy\n",
    "        \n",
    "    return (total_val_loss/len(ValidationLoader), total_val_accuracy/len(ValidationLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs:int = 0, TRAINING_LOADER = TRAINING_LOADER , VALIDATION_LOADER = VALIDATION_LOADER, OPTIM_MODEL_NAME=\"\" ):\n",
    "    all_training_losses = []\n",
    "    all_training_accuracy = []\n",
    "    \n",
    "    all_validation_losses = []\n",
    "    all_validation_accuracy = []\n",
    "    \n",
    "    minimum_validation_loss = torch.tensor(999999999).to(DEVICE)\n",
    "    minimum_validation_epoch_number = -1\n",
    "    \n",
    "    \n",
    "    for epoch in tq(range(epochs)):\n",
    "        total_epoch_loss = 0\n",
    "        total_accuracy_epoch = 0\n",
    "        for i, data in tqdm(enumerate(TRAINING_LOADER, 0)): \n",
    "            \n",
    "            image = 0\n",
    "            label = 0\n",
    "            \n",
    "            image,label = data\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            label = label.to(DEVICE)\n",
    "            image = image.to(DEVICE)\n",
    "            output = net(image)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_epoch_loss += loss\n",
    "                batches_training_accuracy = accuracy_finder(predictions=output, labels=label)\n",
    "                total_accuracy_epoch = total_accuracy_epoch  + batches_training_accuracy   \n",
    "            \n",
    "            if i % 1000 == 0 and i != 0: \n",
    "                print(\"Batch : {}/{}\".format(i, len(TRAINING_LOADER)))\n",
    "           \n",
    "        # total epoch loss \n",
    "        total_epoch_loss = total_epoch_loss / len(TRAINING_LOADER)\n",
    "        # total epoch accuracy \n",
    "        total_accuracy_epoch = total_accuracy_epoch /len(TRAINING_LOADER)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        inputs = 0\n",
    "        image = 0\n",
    "        output = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "             validation_loss, validation_accuracy = validate(ValidationLoader= VALIDATION_LOADER )\n",
    "        \n",
    "        all_validation_losses.append(validation_loss)\n",
    "        all_validation_accuracy.append(validation_accuracy)\n",
    "        \n",
    "        if minimum_validation_loss > validation_loss:\n",
    "            minimum_validation_loss = validation_loss\n",
    "            minimum_validation_epoch_number = epoch\n",
    "            print(\"Saved model at epoch {}\".format(epoch+1))\n",
    "            PATH = OPTIM_MODEL_NAME\n",
    "            # saving model\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}, acc = {:.8f}, val_loss = {:.8f} , val_acc = {:.8f}, \".format(epoch + 1, epochs, total_epoch_loss, total_accuracy_epoch, validation_loss, validation_accuracy ))\n",
    "        all_training_losses.append(total_epoch_loss)\n",
    "        all_training_accuracy.append(total_accuracy_epoch)\n",
    "        \n",
    "    print(\"Training completed\")\n",
    "    return all_training_accuracy, all_training_losses,all_validation_accuracy,all_validation_losses,minimum_validation_epoch_number, minimum_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-04-17 01:56:46.281846\n",
      "date and time = 17/04/2021 01:56:46\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "MODEL_NAME = \"pretrained_resnet_50\"\n",
    "# dd/mm/YY H:M:S\n",
    "ExcecutionTime = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", ExcecutionTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./models\"\n",
    "OPTIM_MODEL_NAME_PATH = \"{}/{}_{}_minimum_validation_loss.pth\".format(PATH,MODEL_NAME,ExcecutionTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728b2d27f2f44818be3f1d894d4b899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [07:35,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 1000/3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [14:39,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 2000/3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [21:42,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 3000/3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3444it [24:50,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at epoch 1\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './models/pretrained_resnet_50_17/04/2021 01:56:46_minimum_validation_loss.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-52e846735a61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val_loss_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAINING_LOADER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTRAINING_LOADER\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mVALIDATION_LOADER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVALIDATION_LOADER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIM_MODEL_NAME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOPTIM_MODEL_NAME_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-a090f96e7345>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(epochs, TRAINING_LOADER, VALIDATION_LOADER, OPTIM_MODEL_NAME)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOPTIM_MODEL_NAME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# saving model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# display the epoch training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './models/pretrained_resnet_50_17/04/2021 01:56:46_minimum_validation_loss.pth'"
     ]
    }
   ],
   "source": [
    "t_acc, t_loss, v_acc, v_loss, min_val_loss, min_val_loss_epoch = training(100, TRAINING_LOADER = TRAINING_LOADER , VALIDATION_LOADER = VALIDATION_LOADER, OPTIM_MODEL_NAME=OPTIM_MODEL_NAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "torch.save(net.state_dict(), MODEL_NAME_PATH)\n",
    "print(\"model was successfully saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_plot(t_value,v_value,title,min_point, ylabel):\n",
    "    result = []\n",
    "    result_v = []\n",
    "    for item, item_v in zip(t_value, v_value):\n",
    "        result.append(item.cpu())\n",
    "        result_v.append(item_v.cpu())\n",
    "    plt.plot(result,'b')\n",
    "    plt.plot(result_v,'r')\n",
    "    if min_point is not None:\n",
    "        x,y = min_point\n",
    "        y = y.cpu()\n",
    "        plt.plot(x,y,'xr',linewidth=2, markersize=12) \n",
    "    plt.title(title)\n",
    "    plt.legend([\"training\",\"validation\"])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_plot(t_value=t_acc,v_value=v_acc,title=\"Accuracy\",min_point=None, ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_plot(t_value=t_loss,v_value=v_loss, title=\"Loss\" ,min_point=(min_val_loss,min_val_loss_epoch), ylabel=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
